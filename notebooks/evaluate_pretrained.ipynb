{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "808487c2",
   "metadata": {},
   "source": [
    "# Evaluate Pre-trained nanoTabPFN\n",
    "This notebook loads a pre-trained model and evaluates it on OpenML datasets, comparing against baseline methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add47b5e",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your trained model\n",
    "MODEL_PATH = \"../trained_model_20241201_120000.pth\"  # Update this path!\n",
    "\n",
    "# Model configuration (should match what was used during training)\n",
    "MODEL_CONFIG = {\n",
    "    \"embedding_size\": 96,\n",
    "    \"num_attention_heads\": 4,\n",
    "    \"mlp_hidden_size\": 192,\n",
    "    \"num_layers\": 3,\n",
    "    \"num_outputs\": 2\n",
    "}\n",
    "\n",
    "# Dataset configuration\n",
    "MAX_FEATURES = 10\n",
    "NUM_INSTANCES = 200\n",
    "TARGET_CLASSES = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3ffc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import seaborn as sns\n",
    "import openml\n",
    "from openml.tasks import TaskType\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder, FunctionTransformer\n",
    "import torch\n",
    "\n",
    "from model import NanoTabPFNModel, NanoTabPFNClassifier\n",
    "from train import get_default_device\n",
    "\n",
    "device = get_default_device()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helper_functions",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=================== DATA LOADING AND PREPROCESSING ===================\n",
    "\"\"\"\n",
    "\n",
    "def get_feature_preprocessor(X: np.ndarray | pd.DataFrame) -> ColumnTransformer:\n",
    "    \"\"\"\n",
    "    fits a preprocessor that imputes NaNs, encodes categorical features and removes constant features\n",
    "    \"\"\"\n",
    "    X = pd.DataFrame(X)\n",
    "    num_mask = []\n",
    "    cat_mask = []\n",
    "    for col in X:\n",
    "        unique_non_nan_entries = X[col].dropna().unique()\n",
    "        if len(unique_non_nan_entries) <= 1:\n",
    "            num_mask.append(False)\n",
    "            cat_mask.append(False)\n",
    "            continue\n",
    "        non_nan_entries = X[col].notna().sum()\n",
    "        numeric_entries = pd.to_numeric(X[col], errors='coerce').notna().sum() # in case numeric columns are stored as strings\n",
    "        num_mask.append(non_nan_entries == numeric_entries)\n",
    "        cat_mask.append(non_nan_entries != numeric_entries)\n",
    "\n",
    "    num_mask = np.array(num_mask)\n",
    "    cat_mask = np.array(cat_mask)\n",
    "\n",
    "    num_transformer = Pipeline([\n",
    "        (\"to_pandas\", FunctionTransformer(lambda x: pd.DataFrame(x) if not isinstance(x, pd.DataFrame) else x)),\n",
    "        (\"to_numeric\", FunctionTransformer(lambda x: x.apply(pd.to_numeric, errors='coerce').to_numpy())),\n",
    "    ])\n",
    "    cat_transformer = Pipeline([\n",
    "        ('encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=np.nan)),\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', num_transformer, num_mask),\n",
    "            ('cat', cat_transformer, cat_mask)\n",
    "        ]\n",
    "    )\n",
    "    return preprocessor\n",
    "\n",
    "def get_openml_datasets(\n",
    "        max_features_eval: int = 10, \n",
    "        new_instances_eval: int = 200, \n",
    "        target_classes_filter: int = 2,\n",
    "        **kwargs,\n",
    "        ) -> dict[str, tuple[np.ndarray, np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Load OpenML tabarena datasets with at most `max_features` features and subsampled (stratified) to `new_instances` instances.\n",
    "    \"\"\"\n",
    "    task_ids = [\n",
    "        363612, 363613, 363614, 363615, 363616, 363618, 363619, 363620,\n",
    "        363621, 363623, 363624, 363625, 363626, 363627, 363628, 363629,\n",
    "        363630, 363631, 363632, 363671, 363672, 363673, 363674, 363675,\n",
    "        363676, 363677, 363678, 363679, 363681, 363682, 363683, 363684,\n",
    "        363685, 363686, 363689, 363691, 363693, 363694, 363696, 363697,\n",
    "        363698, 363699, 363700, 363702, 363704, 363705, 363706, 363707,\n",
    "        363708, 363711, 363712\n",
    "    ] # TabArena v0.1\n",
    "    datasets = {}\n",
    "    for task_id in task_ids:\n",
    "        task = openml.tasks.get_task(task_id, download_splits=False)\n",
    "        if task.task_type_id != TaskType.SUPERVISED_CLASSIFICATION:\n",
    "            continue  # skip task, only classification\n",
    "        dataset = task.get_dataset(download_data=False)\n",
    "\n",
    "        if dataset.qualities[\"NumberOfFeatures\"] > max_features_eval or (dataset.qualities[\"NumberOfClasses\"] > target_classes_filter) or dataset.qualities[\"PercentageOfInstancesWithMissingValues\"] > 0 or dataset.qualities[\"MinorityClassPercentage\"] < 2.5:\n",
    "            continue\n",
    "        X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
    "            target=task.target_name, dataset_format=\"dataframe\"\n",
    "        )\n",
    "        if new_instances_eval < len(y):\n",
    "            _, X_sub, _, y_sub = train_test_split(\n",
    "                X, y,\n",
    "                test_size=new_instances_eval,\n",
    "                stratify=y,\n",
    "                random_state=0,\n",
    "            )\n",
    "        else:\n",
    "            X_sub = X\n",
    "            y_sub = y\n",
    "        \n",
    "        X = X_sub.to_numpy(copy=True)\n",
    "        y = y_sub.to_numpy(copy=True)\n",
    "        label_encoder = LabelEncoder()\n",
    "        y = label_encoder.fit_transform(y)\n",
    "\n",
    "        preprocessor = get_feature_preprocessor(X)\n",
    "        X = preprocessor.fit_transform(X)\n",
    "        datasets[dataset.name] = (X, y)\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval_functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=================== EVALUATION ===================\n",
    "\"\"\"\n",
    "\n",
    "_skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "def eval_model(model, datasets):\n",
    "    \"\"\"Evaluates a model on multiple datasets and returns metrics\"\"\"\n",
    "    metrics = {}\n",
    "    for dataset_name, (X,y)  in datasets.items():\n",
    "        targets = []\n",
    "        probabilities = []\n",
    "        \n",
    "        for train_idx, test_idx in _skf.split(X, y):\n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "            y_train, y_test  = y[train_idx], y[test_idx]\n",
    "            targets.append(y_test)\n",
    "            model.fit(X_train, y_train)\n",
    "            y_proba = model.predict_proba(X_test)\n",
    "            if y_proba.shape[1] == 2:  # binary classification with neural network\n",
    "                y_proba = y_proba[:, 1]\n",
    "            probabilities.append(y_proba)\n",
    "    \n",
    "        targets = np.concatenate(targets, axis=0)\n",
    "        probabilities = np.concatenate(probabilities, axis=0)\n",
    "\n",
    "        metrics[f\"{dataset_name}/ROC AUC\"] = roc_auc_score(targets, probabilities, multi_class=\"ovr\")\n",
    "    \n",
    "    metric_names = list({key.split(\"/\")[-1] for key in metrics.keys()})\n",
    "    for metric_name in metric_names:\n",
    "        avg_metric = np.mean([metrics[key] for key in metrics.keys() if key.endswith(metric_name)])\n",
    "        metrics[f\"{metric_name}\"] = float(avg_metric)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_model",
   "metadata": {},
   "source": [
    "## Load Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_model_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model with the same configuration used during training\n",
    "model = NanoTabPFNModel(**MODEL_CONFIG)\n",
    "\n",
    "# Load the trained weights\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Create classifier wrapper\n",
    "nano_classifier = NanoTabPFNClassifier(model, device)\n",
    "\n",
    "print(f\"Successfully loaded model from {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_datasets",
   "metadata": {},
   "source": [
    "## Load OpenML Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_datasets_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading OpenML datasets...\")\n",
    "DATASETS = get_openml_datasets(\n",
    "    max_features_eval=MAX_FEATURES,\n",
    "    new_instances_eval=NUM_INSTANCES,\n",
    "    target_classes_filter=TARGET_CLASSES\n",
    ")\n",
    "print(f\"Loaded {len(DATASETS)} datasets: {list(DATASETS.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval_section",
   "metadata": {},
   "source": [
    "## Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval_nano",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating nanoTabPFN...\")\n",
    "nano_results = eval_model(nano_classifier, DATASETS)\n",
    "print(\"\\nnanoTabPFN Results:\")\n",
    "print(nano_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval_baselines",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tabpfn import TabPFNClassifier\n",
    "from tabpfn.config import ModelInterfaceConfig, PreprocessorConfig\n",
    "\n",
    "print(\"Evaluating baseline models...\")\n",
    "\n",
    "no_preprocessing_inference_config = ModelInterfaceConfig(\n",
    "    FINGERPRINT_FEATURE=False,\n",
    "    PREPROCESS_TRANSFORMS=[PreprocessorConfig(name='none')]\n",
    ")\n",
    "\n",
    "baseline_models = {\n",
    "    \"TabPFN v2\": TabPFNClassifier(random_state=0),\n",
    "    \"TabPFN v2 (no preprocessing)\": TabPFNClassifier(\n",
    "        inference_config=no_preprocessing_inference_config, \n",
    "        n_estimators=1, \n",
    "        random_state=0\n",
    "    ),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=0),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=0),\n",
    "}\n",
    "\n",
    "baseline_results = {}\n",
    "for name, model in baseline_models.items():\n",
    "    print(f\"  Evaluating {name}...\")\n",
    "    baseline_results[name] = eval_model(model, DATASETS)\n",
    "\n",
    "print(\"\\nBaseline evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results_section",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results_table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results DataFrame\n",
    "all_results = {\"nanoTabPFN\": nano_results, **baseline_results}\n",
    "results_df = pd.DataFrame(all_results).T\n",
    "\n",
    "# Sort by ROC AUC descending\n",
    "results_df = results_df.sort_values(\"ROC AUC\", ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OVERALL RESULTS (Average ROC AUC across all datasets)\")\n",
    "print(\"=\"*80)\n",
    "print(results_df[[\"ROC AUC\"]].to_string())\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Display full results\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plots_section",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_overall",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot overall comparison\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"notebook\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Create bar plot\n",
    "colors = ['#1f77b4' if idx == 'nanoTabPFN' else '#d62728' for idx in results_df.index]\n",
    "results_df[\"ROC AUC\"].plot(kind='barh', ax=ax, color=colors)\n",
    "\n",
    "ax.set_xlabel('ROC AUC', fontsize=12)\n",
    "ax.set_ylabel('Model', fontsize=12)\n",
    "ax.set_title('Model Comparison on OpenML TabArena Datasets', fontsize=14, fontweight='bold')\n",
    "ax.axvline(x=results_df[\"ROC AUC\"].loc[\"nanoTabPFN\"], color='blue', linestyle='--', alpha=0.3)\n",
    "ax.set_xlim(results_df[\"ROC AUC\"].min() - 0.02, results_df[\"ROC AUC\"].max() + 0.02)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_per_dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot per-dataset comparison\n",
    "dataset_metrics = [col for col in results_df.columns if \"/\" in col]\n",
    "dataset_names = [col.split(\"/\")[0] for col in dataset_metrics]\n",
    "\n",
    "# Create subplot for each dataset\n",
    "n_datasets = len(dataset_names)\n",
    "n_cols = 3\n",
    "n_rows = (n_datasets + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4 * n_rows))\n",
    "axes = axes.flatten() if n_datasets > 1 else [axes]\n",
    "\n",
    "for idx, (dataset_name, metric_col) in enumerate(zip(dataset_names, dataset_metrics)):\n",
    "    ax = axes[idx]\n",
    "    dataset_results = results_df[metric_col].sort_values(ascending=True)\n",
    "    colors = ['#1f77b4' if model == 'nanoTabPFN' else '#d62728' for model in dataset_results.index]\n",
    "    dataset_results.plot(kind='barh', ax=ax, color=colors)\n",
    "    ax.set_title(dataset_name, fontsize=10, fontweight='bold')\n",
    "    ax.set_xlabel('ROC AUC', fontsize=9)\n",
    "    ax.set_ylabel('')\n",
    "    ax.tick_params(axis='y', labelsize=8)\n",
    "\n",
    "# Hide unused subplots\n",
    "for idx in range(n_datasets, len(axes)):\n",
    "    axes[idx].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save_results",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_results_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "os.makedirs(\"../results\", exist_ok=True)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "results_filename = f\"../results/pretrained_evaluation_{timestamp}.csv\"\n",
    "results_df.to_csv(results_filename)\n",
    "print(f\"Results saved to: {results_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
